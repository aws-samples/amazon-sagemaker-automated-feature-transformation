{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating feature transformations with SageMaker Data Wrangler, Pipelines, and Feature Store\n",
    "\n",
    "1. Add permissions to Amazon SageMaker\n",
    "2. Create a data wrangler flow\n",
    "3. Export the flow and create feature groups\n",
    "4. Ingest historical data into feature store\n",
    "5. Set up SageMaker pipeline and Lambdas triggered off new data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the blog post will walk the reader through creating a set of feature transformations on the flight delay dataset and then export that flow to a generated notebook that creates feature groups and ingests historical flight data into the feature store. This notebook forms the second half of the example, where readers will create a SageMaker pipeline and a lambda function to automate the feature transformations and feature store ingest on new data each day."
   ]
  },
  {
   "source": [
    "## Add permissions for Amazon SageMaker\n",
    "As part of automation in this notebook, you will create IAM roles to assign to AWS Lambda. To do that, you first need to give permission to Amazon SageMaker to create and manage IAM roles. You can provide those permissions by adding the following permissions as an [inline policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-edit.html#edit-inline-policy-console) to your Amazon SageMaker role."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"IAMPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:CreatePolicy\",\n",
    "                \"iam:AttachRolePolicy\",\n",
    "                \"iam:CreateRole\",\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ],\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": [\n",
    "                        \"lambda.amazonaws.com\",\n",
    "                        \"sagemaker.amazonaws.com\"\n",
    "                        ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"LambdaFunction\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:CreateFunction\",\n",
    "                \"lambda:UpdateFunctionCode\",\n",
    "                \"lambda:AddPermission\",\n",
    "                \"sts:GetCallerIdentity\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SM Pipeline from the Data Wrangler Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.20.0\":\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.20.0\"]\n",
    "    )\n",
    "    import importlib\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update S3 Bucket to the one cotaining dataset and flow file.\n",
    "bucket = None\n",
    "if bucket is None:\n",
    "   raise RuntimeError(\"Please add name of the bucket that contains the data and flow files.\")\n",
    "\n",
    "# Update Flow URI from 4th cell of the Jupyter notebook that was exported by Amazon SageMaker Data Wrangler.\n",
    "flow_uri = None\n",
    "if flow_uri is None:\n",
    "   raise RuntimeError(\"Please add flow URI from the Jupyter notebook that was exported by Amazon SageMaker Data Wrangler.\")\n",
    "\n",
    "# Update Feature Group name from 5th cell of the Jupyter notebook that was exported by Amazon SageMaker Data Wrangler.\n",
    "feature_group_name = None\n",
    "if feature_group_name is None:\n",
    "   raise RuntimeError(\"Please add Feature Group name from the Jupyter notebook that was exported by Amazon SageMaker Data Wrangler.\")\n",
    "\n",
    "# If you updated prefix in the other Jupyter notebook, update it here as well.\n",
    "prefix = \"data_wrangler_flows\"\n",
    "flow_id = ((((flow_uri.split(prefix))[1])[1:]).split('.'))[0]\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "container_uri = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.1.1\"\n",
    "\n",
    "# Processing Job Resources Configurations\n",
    "processing_job_name = f\"data-wrangler-feature-store-processing-{flow_id}\"\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "\n",
    "# URL to use for sagemaker client.\n",
    "# If this is None, boto will automatically construct the appropriate URL to use when communicating with sagemaker.\n",
    "sagemaker_endpoint_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.4xlarge\"\n",
    ")\n",
    "input_flow= ParameterString(\n",
    "    name='InputFlow',\n",
    "    default_value=flow_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import FeatureStoreOutput\n",
    "feature_store_output = FeatureStoreOutput(feature_group_name=feature_group_name)\n",
    "feature_store_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = \"26b67c19-4e8b-401b-a817-db82c20a17ed.default\"\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "def create_container_arguments(output_name, output_content_type):\n",
    "    output_config = {\n",
    "        output_name: {\n",
    "            \"content_type\": output_content_type\n",
    "        }\n",
    "    }\n",
    "    return [f\"--output-config '{json.dumps(output_config)}'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"DailyFlightDataETL\",\n",
    "    processor=processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(input_name='flow', \n",
    "                        destination='/opt/ml/processing/flow',\n",
    "                        source=input_flow,\n",
    "                        s3_data_type= 'S3Prefix',\n",
    "                        s3_input_mode= 'File'\n",
    "                       )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"26b67c19-4e8b-401b-a817-db82c20a17ed.default\",\n",
    "            app_managed=True, feature_store_output= feature_store_output)\n",
    "    ],\n",
    "   job_arguments=create_container_arguments(output_name, output_content_type)\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name='dw-fs-automation'\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        input_flow\n",
    "    ],\n",
    "    steps=[step_process],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution=pipeline.start(execution_description='removing all ref to output name',\n",
    "                        parameters={\n",
    "                            'InputFlow':flow_uri\n",
    "                                   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "import inspect\n",
    "import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Variables\n",
    "#### We now set variables that will be used to setup the automation. The default placeholder values will work but you can update them as well, if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role_name = f\"sm-lambda-role-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "fcn_name = f\"sm-lambda-fcn-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "iam_desc = 'IAM Policy for Lambda triggering AWS SageMaker Pipeline'\n",
    "fcn_desc = 'AWS Lambda function for automatically triggering AWS SageMaker Pipeline'\n",
    "bucket_arn = f\"arn:aws:s3:::{bucket}\"\n",
    "account_num = boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup IAM Roles\n",
    "#### AWS Lambda needs permissions to be able to call other AWS services. These permissions are provided by IAM roles. We first create the IAM role that will be assumed by AWS Lambda and then assign permissions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create IAM role for the Lambda function\n",
    "new_role = Utils.create_role(role_name, iam_desc)\n",
    "\n",
    "#Wait for IAM role to be active\n",
    "print('Pause for 10 seconds ...')\n",
    "for i in range(10,0,-1):\n",
    "    time.sleep(1)\n",
    "    print('Resuming in {} seconds'.format(i))\n",
    "print('Resuming now!')\n",
    "#Add permissions to the IAM role\n",
    "Utils.add_permissions(new_role['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup AWS Lambda function\n",
    "#### We need AWS Lambda to automtically trigger Amazon SageMaker Pipelines to process newly arrived dataset in Amazon S3. The detailed code is available in `Utils.py`. Once the AWS Lambda function is created, we zip it into a deployment package ready for upload onto AWS Lambda. Once the package is ready, we create the AWS Lambda function using the IAM role created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "#Create code for AWS Lambda function\n",
    "lambda_code = Utils.create_lambda_fcn(bucket, flow_uri, pipeline_name)\n",
    "\n",
    "#Zip AWS Lambda function code\n",
    "#Write code to a .py file\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(inspect.cleandoc(lambda_code))\n",
    "#Compress file into a zip\n",
    "with ZipFile('function.zip','w') as z:\n",
    "    z.write('lambda_function.py')\n",
    "#Use zipped code as AWS Lambda function code\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(lambda_code)\n",
    "\n",
    "#Create AWS Lambda function\n",
    "with open('function.zip', 'rb') as f:\n",
    "    fcn_code = f.read()   \n",
    "new_lambda_arn = Utils.create_lambda(fcn_name, fcn_desc, fcn_code, new_role['arn'])"
   ]
  },
  {
   "source": [
    "### Setup Amazon S3\n",
    "#### Lastly, we setup Amazon S3 to trigger AWS Lambda whenever a new CSV file is uploaded into the Bucket specified earlier. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add permission for Amazon S3 to trigger AWS Lambda\n",
    "Utils.allow_s3(fcn_name, bucket_arn, account_num)\n",
    "\n",
    "#Setup new CSV upload notifications on Amazon S3\n",
    "Utils.add_notif(bucket, new_lambda_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion\n",
    "#### You have now successfully setup the automation. Try and test the setup by uploading a CSV file into your Amazon S3 Bucket and see if it triggers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print Confirmation\n",
    "print('GOOD JOB: You are all set!')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit4fd9a88ec5964e36b034f674defd5e81"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}