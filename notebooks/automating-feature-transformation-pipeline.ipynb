{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating feature transformations with SageMaker Data Wrangler, Pipelines, and Feature Store\n",
    "\n",
    "1. Create a data wrangler flow\n",
    "2. Export the flow and create feature groups\n",
    "3. Ingest historical data into feature store\n",
    "4. Set up SageMaker pipeline and Lambdas triggered off new data from S3\n",
    "5. (maybe) Build and deploy model to show e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of the blog post will walk the reader through creating a set of feature transformations on the flight delay dataset and then export that flow to a generated notebook that creates feature groups and ingests historical flight data into the feature store. This notebook forms the second half of the example, where readers will create a SageMaker pipeline and a lambda function to automate the feature transformations and feature store ingest on new data each day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SM Pipeline from the Data Wrangler Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.20.0\":\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.20.0\"]\n",
    "    )\n",
    "    import importlib\n",
    "    importlib.reload(sagemaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-926082456644\n",
      "s3://sagemaker-us-east-1-926082456644/data_wrangler_flows/flow-16-15-49-59-82ce8678.flow\n",
      "data_wrangler_flows/flow-16-15-49-59-82ce8678.flow\n"
     ]
    }
   ],
   "source": [
    "# S3 bucket for saving processing job outputs\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = \"data_wrangler_flows\"\n",
    "flow_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_name = f\"flow-{flow_id}\"\n",
    "flow_uri = f\"s3://{bucket}/{prefix}/{flow_name}.flow\"\n",
    "print(bucket)\n",
    "print(flow_uri)\n",
    "print(prefix + '/' + flow_name + '.flow')\n",
    "\n",
    "flow_file_name = \"flight_data_test_2.flow\"\n",
    "\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "container_uri = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.1.1\"\n",
    "\n",
    "# Processing Job Resources Configurations\n",
    "processing_job_name = f\"data-wrangler-feature-store-processing-{flow_id}\"\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "feature_group_name = \"FG-flow-14-18-14-03-23a5ae4a\"\n",
    "\n",
    "# URL to use for sagemaker client.\n",
    "# If this is None, boto will automatically construct the appropriate URL to use\n",
    "# when communicating with sagemaker.\n",
    "sagemaker_endpoint_url = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to do \n",
    "\n",
    "## grab feature group name, save as variable\n",
    "##Â have reader copy and paste output_name for proc job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.4xlarge\"\n",
    ")\n",
    "#input_data = ParameterString(\n",
    " #   name=\"InputData\",\n",
    "    #must have S3 URI for default_value\n",
    "  #  default_value='s3://sagemaker-studio-83hjatr6mug/test_data/fake.csv'\n",
    "#)\n",
    "input_flow= ParameterString(\n",
    "    name='InputFlow',\n",
    "    default_value='s3://sagemaker-studio-83hjatr6mug/test_data/fake.flow'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureStoreOutput(feature_group_name='FG-flow-14-18-14-03-23a5ae4a')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.processing import FeatureStoreOutput\n",
    "feature_store_output = FeatureStoreOutput(feature_group_name=feature_group_name)\n",
    "feature_store_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name = \"26b67c19-4e8b-401b-a817-db82c20a17ed.default\"\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# are container arguments needed?\n",
    "\n",
    "def create_container_arguments(output_name, output_content_type):\n",
    "    output_config = {\n",
    "        output_name: {\n",
    "            \"content_type\": output_content_type\n",
    "        }\n",
    "    }\n",
    "    return [f\"--output-config '{json.dumps(output_config)}'\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"DailyFlightDataETL\",\n",
    "    processor=processor,\n",
    "    #inputs=create_processing_inputs(processing_dir, flow, flow_uri),\n",
    "    inputs=[\n",
    "        ProcessingInput(input_name='flow', \n",
    "                        destination='/opt/ml/processing/flow',\n",
    "                        #source='s3://sagemaker-us-east-1-926082456644/data_wrangler_flows/flow-14-13-21-25-aaf22ec2.flow',\n",
    "                        source=input_flow,\n",
    "                        s3_data_type= 'S3Prefix',\n",
    "                        s3_input_mode= 'File'\n",
    "                       )\n",
    "        #ProcessingInput(input_name='input_csv',\n",
    "                       # destination='/opt/ml/processing/input',\n",
    "                       # source=input_data,\n",
    "                       # s3_data_type= 'S3Prefix',\n",
    "                       # s3_input_mode= 'File')\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"26b67c19-4e8b-401b-a817-db82c20a17ed.default\",\n",
    "            app_managed=True, feature_store_output= feature_store_output)\n",
    "    ],\n",
    "   job_arguments=create_container_arguments(output_name, output_content_type)\n",
    "   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name='dw-fs-test-2'\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        #input_data,\n",
    "        input_flow\n",
    "    ],\n",
    "    steps=[step_process],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:926082456644:pipeline/dw-fs-test-2',\n",
       " 'ResponseMetadata': {'RequestId': '4c5cb65a-bac6-4172-86b6-b9c4a6d5b632',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4c5cb65a-bac6-4172-86b6-b9c4a6d5b632',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '80',\n",
       "   'date': 'Tue, 16 Mar 2021 15:50:01 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(sagemaker.get_execution_role())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#flow_uri='s3://sagemaker-us-east-1-926082456644/data_wrangler_flows/new_source_flow.flow'\n",
    "flow_uri='s3://sagemaker-us-east-1-926082456644/data_wrangler_flows/flow-14-13-21-25-aaf22ec2.flow'\n",
    "\n",
    "execution=pipeline.start(execution_description='removing all ref to output name',\n",
    "                        parameters={\n",
    "                            'InputFlow':flow_uri\n",
    "                                   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "import inspect\n",
    "import Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Variables\n",
    "#### We now set variables that will be used to setup the automation. The default placeholder values will work but you can update them as well, if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_group_name = \"FG-flow-10-15-59-59-79694afa\"\n",
    "# output_name = \"49ca6759-4428-4991-be63-805b3d283301.default\"\n",
    "# flow_name = \"flow-10-15-59-59-79694afa.flow\"\n",
    "role_name = f\"sm-lambda-role-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "fcn_name = f\"sm-lambda-fcn-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "iam_desc = 'IAM Policy for Lambda triggering AWS SageMaker Pipeline'\n",
    "fcn_desc = 'AWS Lambda function for automatically triggering AWS SageMaker Pipeline'\n",
    "bucket_arn = f\"arn:aws:s3:::{bucket}\"\n",
    "account_num = boto3.client('sts').get_caller_identity()['Account']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup IAM Roles\n",
    "#### AWS Lambda needs permissions to be able to call other AWS services. These permissions are provided by IAM roles. We first create the IAM role that will be assumed by AWS Lambda and then assign permissions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating an IAM role for AWS Lambda function ...\n",
      "SUCCESS: Successfully created IAM role for AWS Lambda function!\n",
      "Pause for 10 seconds ...\n",
      "Resuming in 10 seconds\n",
      "Resuming in 9 seconds\n",
      "Resuming in 8 seconds\n",
      "Resuming in 7 seconds\n",
      "Resuming in 6 seconds\n",
      "Resuming in 5 seconds\n",
      "Resuming in 4 seconds\n",
      "Resuming in 3 seconds\n",
      "Resuming in 2 seconds\n",
      "Resuming in 1 seconds\n",
      "Resuming now!\n",
      "Adding permissions to AWS Lambda function's IAM role ...\n",
      "SUCCESS: Successfully added permissions AWS Lambda function's IAM role!\n"
     ]
    }
   ],
   "source": [
    "#Create IAM role for the Lambda function\n",
    "new_role = Utils.create_role(role_name, iam_desc)\n",
    "\n",
    "#Wait for IAM role to be active\n",
    "print('Pause for 10 seconds ...')\n",
    "for i in range(10,0,-1):\n",
    "    time.sleep(1)\n",
    "    print('Resuming in {} seconds'.format(i))\n",
    "print('Resuming now!')\n",
    "#Add permissions to the IAM role\n",
    "Utils.add_permissions(new_role['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup AWS Lambda function\n",
    "#### We need AWS Lambda to automtically trigger Amazon SageMaker Pipelines to process newly arrived dataset in Amazon S3. The detailed code is available in `Utils.py`. Once the AWS Lambda function is created, we zip it into a deployment package ready for upload onto AWS Lambda. Once the package is ready, we create the AWS Lambda function using the IAM role created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering variables ...\n",
      "Creating code for AWS Lambda function ...\n",
      "SUCCESS: Successfully created code for AWS Lambda function!\n",
      "Creating AWS Lambda function ...\n",
      "SUCCESS: Successfully created AWS Lambda function!\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "#Create code for AWS Lambda function\n",
    "lambda_code = Utils.create_lambda_fcn(bucket, flow_uri, pipeline_name)\n",
    "\n",
    "#Zip AWS Lambda function code\n",
    "#Write code to a .py file\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(inspect.cleandoc(lambda_code))\n",
    "#Compress file into a zip\n",
    "with ZipFile('function.zip','w') as z:\n",
    "    z.write('lambda_function.py')\n",
    "#Use zipped code as AWS Lambda function code\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(lambda_code)\n",
    "\n",
    "#Create AWS Lambda function\n",
    "with open('function.zip', 'rb') as f:\n",
    "    fcn_code = f.read()   \n",
    "new_lambda_arn = Utils.create_lambda(fcn_name, fcn_desc, fcn_code, new_role['arn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup Amazon S3\n",
    "#### Lastly, we setup Amazon S3 to trigger AWS Lambda whenever a new CSV file is uploaded into the Bucket specified earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding permissions to Amazon S3 ...\n",
      "SUCCESS: Successfully added permissions to Amazon S3!\n",
      "Initialising Amazon S3 Bucket client ...\n",
      "SUCCESS: Successfully initilised Amazon S3 Bucket client!\n",
      "Setting up notifications on Amazon S3 Bucket\n",
      "SUCCESS: Successfully added notifications to Amazon S3 Bucket!\n"
     ]
    }
   ],
   "source": [
    "#Add permission for Amazon S3 to trigger AWS Lambda\n",
    "Utils.allow_s3(fcn_name, bucket_arn, account_num)\n",
    "\n",
    "#Setup new CSV upload notifications on Amazon S3\n",
    "Utils.add_notif(bucket, new_lambda_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion\n",
    "#### You have now successfully setup the automation. Try and test the setup by uploading a CSV file into your Amazon S3 Bucket and see if it triggers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD JOB: You are all set!\n"
     ]
    }
   ],
   "source": [
    "#Print Confirmation\n",
    "print('GOOD JOB: You are all set!')"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
