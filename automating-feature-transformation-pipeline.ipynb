{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating feature transformations with SageMaker Data Wrangler, Pipelines, and Feature Store\n",
    "\n",
    "This notebook shows you how to create a SageMaker Pipeline along with an AWS Lambda function to automate feature transformations and ingestion into Feature Store, triggered off of new data files that are uploaded to S3. It assumes that you already have already created a Data Wrangler `.flow` file, which is the main output of the first half of the steps described it this blog post. \n",
    "\n",
    "The notebook has three main sections:\n",
    "1.  General setup\n",
    "2.\tCreating a SageMaker Pipeline which:\n",
    "    - Performs the transformations contained in a Data Wrangler `.flow` file stored in Amazon S3 using a SageMaker Processing Job \n",
    "    - Stores the transformed features in the Amazon SageMaker Feature Store\n",
    "3.\tCreating an AWS Lambda function which:\n",
    "    - Is triggered whenever any new data is uploaded to S3\n",
    "    - Updates the `.flow` file to reference the new dataset\n",
    "    - Triggers the SageMaker Pipeline with the new `.flow` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to copy these variables from the Data Wrangler generated notebook from the previous step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group_name = \"<FEATURE GROUP NAME>\"\n",
    "output_name = \"<OUTPUT NAME>\"\n",
    "flow_uri='<FLOW URI>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker Python SDK version 2.x is required\n",
    "import sagemaker\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from zipfile import ZipFile\n",
    "import inspect\n",
    "\n",
    "#module containing utility functions for this notebook\n",
    "import pipeline_utils\n",
    "\n",
    "original_version = sagemaker.__version__\n",
    "if sagemaker.__version__ != \"2.20.0\":\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"sagemaker==2.20.0\"]\n",
    "    )\n",
    "    import importlib\n",
    "    importlib.reload(sagemaker)\n",
    "    \n",
    "# S3 bucket for saving processing job outputs\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "sm_client = boto3.client('sagemaker')\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "region = sess.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Historical Records in Feature Store\n",
    "First, let's ensure that the records we processed in previous step made it into the Feature Store successfully. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_utils.get_historical_record_count(feature_group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update policy of SageMaker Studio execution role \n",
    "As part of automation in this notebook, you will create IAM roles to assign to AWS Lambda. To do that, you first need to give some permission to am IAM execution role. You can provide those permissions by adding the following as an [inline policy](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_manage-edit.html#edit-inline-policy-console).\n",
    "\n",
    "If you are running this notebook in Amazon SageMaker Studio, the IAM role assumed by your Studio user needs permission to create AWS Lambda functions and IAM roles. To provide this permission to the role, do the following.\n",
    "\n",
    "1. Open the [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/).\n",
    "2. Select Amazon SageMaker Studio and choose your user name.\n",
    "3. Under **User summary**, copy just the name part of the execution role ARN \n",
    "5. Go to the [IAM console](https://console.aws.amazon.com/iam) and click on **Roles**. \n",
    "6. Find the role associated with your SageMaker Studio user\n",
    "7. Under the Permissions tab, click **Add inline policy** and enter the following in the JSON tab:\n",
    "```\n",
    "{   \n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"IAMPolicy\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:CreatePolicy\",\n",
    "                \"iam:AttachRolePolicy\",\n",
    "                \"iam:CreateRole\",\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\"\n",
    "            ],\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": [\n",
    "                        \"lambda.amazonaws.com\",\n",
    "                        \"sagemaker.amazonaws.com\"\n",
    "                        ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Sid\": \"LambdaFunction\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"lambda:CreateFunction\",\n",
    "                \"lambda:UpdateFunctionCode\",\n",
    "                \"lambda:AddPermission\",\n",
    "                \"sts:GetCallerIdentity\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SageMaker Pipeline from the Data Wrangler Flow\n",
    "The transformations we defined in Data Wrangler are encapsulated in a `.flow` file. We will parameterize our SageMaker pipeline with the S3 URI of a new input flow file we will create on the fly once new data is made available in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\",\n",
    "    default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.4xlarge\"\n",
    ")\n",
    "\n",
    "input_flow= ParameterString(\n",
    "    name='InputFlow',\n",
    "    default_value='s3://placeholder-bucket/placeholder.flow'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import Processor\n",
    "\n",
    "container_uri=f\"663277389841.dkr.ecr.{region}.amazonaws.com/sagemaker-data-wrangler-container:1.x\"\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import FeatureStoreOutput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "    \n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"DailyFlightDataETL\",\n",
    "    processor=processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(input_name='flow', \n",
    "                        destination='/opt/ml/processing/flow',\n",
    "                        source=input_flow,\n",
    "                        s3_data_type= 'S3Prefix',\n",
    "                        s3_input_mode= 'File'\n",
    "                       )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=output_name,\n",
    "            app_managed=True, \n",
    "            feature_store_output=FeatureStoreOutput(feature_group_name=feature_group_name))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name=f\"daily-flight-ETL-pipeline-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type, \n",
    "        processing_instance_count,\n",
    "        input_flow\n",
    "    ],\n",
    "    steps=[step_process],\n",
    "    sagemaker_session=sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(iam_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Pipeline set up ready to execute when called with a new input flow file. Now we'll create a lambda function that will automatically create a new flow file when new data is uploaded to S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Lambda Function triggered off of S3\n",
    "We have a provided a file `pipeline_utils.py` which contains some helper functions we can use to create a lambda function containing our custom code. \n",
    "\n",
    "### Setup IAM Roles\n",
    "AWS Lambda needs permissions to be able to call other AWS services. These permissions are provided by IAM roles. We first create the IAM role that will be assumed by AWS Lambda and then assign permissions to it.\n",
    "\n",
    "We now set variables that will be used to setup the automation. The default placeholder values will work but you can update them as well, if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='daily_data'\n",
    "\n",
    "role_name = f\"sm-lambda-role-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "fcn_name = f\"sm-lambda-fcn-{time.strftime('%d-%H-%M-%S', time.gmtime())}\"\n",
    "\n",
    "account_num = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "#Create IAM role for the Lambda function\n",
    "lambda_role = pipeline_utils.create_role(role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Lambda function is created, we zip it into a deployment package ready for upload onto AWS Lambda. Once the package is ready, we create the AWS Lambda function using the IAM role created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create code for AWS Lambda function\n",
    "lambda_code = pipeline_utils.create_lambda_fcn(flow_uri, pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lambda_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zip AWS Lambda function code\n",
    "#Write code to a .py file\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(inspect.cleandoc(lambda_code))\n",
    "#Compress file into a zip\n",
    "with ZipFile('function.zip','w') as z:\n",
    "    z.write('lambda_function.py')\n",
    "#Use zipped code as AWS Lambda function code\n",
    "with open('lambda_function.py', 'w') as f:\n",
    "    f.write(lambda_code)\n",
    "\n",
    "#Create AWS Lambda function\n",
    "with open('function.zip', 'rb') as f:\n",
    "    fcn_code = f.read()   \n",
    "lambda_arn = pipeline_utils.create_lambda(fcn_name, fcn_code, lambda_role['arn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we setup Amazon S3 to trigger AWS Lambda whenever a new CSV file is uploaded into the S3 bucket under the `prefix` specified earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add permission for Amazon S3 to trigger AWS Lambda and set up trigger\n",
    "pipeline_utils.create_s3_trigger(fcn_name, bucket, prefix, account_num, lambda_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now successfully setup up all the necessary pieces of infrastructure. Now we will try and test the setup by uploading a CSV file into your Amazon S3 Bucket and monitor the pipeline execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do \n",
    "download `mar31_2020.csv` locally from public S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.s3.S3Uploader.upload(\"./mar31_2020.csv\", f\"s3://{bucket}/{prefix}\")\n",
    "#wait for file to finish uploading \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check pipeline execution \n",
    "latest_execution = sm_client.list_pipeline_executions(PipelineName=pipeline_name).get('PipelineExecutionSummaries')[0].get('PipelineExecutionArn')\n",
    "sm_client.describe_pipeline_execution(PipelineExecutionArn=latest_execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also monitor the pipeline run inside the Pipelines section of Studio. Once the execution completes, we can check the record in Feature Store for the flight AA1538 from Denver CO to Los Angeles CA from 31 March 2020:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_id='1538DEN707LAX'\n",
    "sample_record = sess.boto_session.client('sagemaker-featurestore-runtime', region_name=region).get_record(FeatureGroupName=feature_group_name, RecordIdentifierValueAsString=str(record_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up\n",
    "To avoid recurring charges you need to: \n",
    "* Delete the datasets uploaded to S3\n",
    "* Stop any running Data Wrangler and Jupyter Notebook instances within Studio when not in use. \n",
    "* Delete the feature group \n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}